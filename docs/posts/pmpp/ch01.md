---
title: "PMPP Ch01: Introduction"
date: 2026-02-19
---

# Ch01: Introduction

<p style="color: var(--vp-c-text-2); font-size: 14px;">
2026-02-19 &nbsp;·&nbsp; PMPP 专栏 &nbsp;·&nbsp; 第一章
</p>

> **本章信息**
> - **书名**: Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)
> - **作者**: David B. Kirk, Wen-mei W. Hwu
> - **出版**: Morgan Kaufmann, 2022
> - **范围**: 第 1 章 Introduction

## 一句话总结

本章回答一个根本问题：**为什么我们需要 GPU 并行计算？** 从处理器频率增长停滞、应用对算力的无尽需求，到 CPU 与 GPU 截然不同的设计哲学，再到 CUDA 异构编程模型的诞生——这一章建立了整本书的"世界观"。

---

## 1.1 异构并行计算：背景与动机

### 频率时代的终结

过去几十年，程序员享受着一种"免费午餐"——**即使不修改代码，程序也会随着处理器频率的提升自动变快**。从 1980 年代到 2000 年代初，CPU 主频从 MHz 飙升到 GHz，应用性能水涨船高。

然而，这个时代在 2003 年前后戛然而止。

```
CPU 主频增长趋势:

频率 (GHz)
4.0 ┤                              ╭─── 频率墙 ───╮
    │                         ●────●────●────●────●   ← 停滞在 ~3-4 GHz
3.0 ┤                    ●
    │               ●
2.0 ┤          ●
    │     ●
1.0 ┤●
    │
0.0 ┼────┬────┬────┬────┬────┬────┬────┬────┬────┬──
   1995 1997 1999 2001 2003 2005 2007 2009 2011 2013

                    ↑
              ~2003年: 频率增长停滞
```

停滞的根本原因是 **功耗墙（Power Wall）**。处理器的动态功耗与频率和电压的关系为：

$$
P_{\text{dynamic}} \propto C \cdot V^2 \cdot f
$$

其中 \(C\) 是电容负载，\(V\) 是供电电压，\(f\) 是时钟频率。要提高频率，通常需要同步提高电压，导致功耗以 **立方** 速率增长。当功耗密度接近物理散热极限时，频率就无法继续提升了。

::: warning 功耗墙的本质
频率提升 → 电压必须同步提升 → 功耗 \(\propto V^2 f \approx f^3\) 增长 → 散热跟不上 → 频率封顶。这不是工程难题，而是物理定律的限制。
:::

### 从频率竞赛到核心竞赛

频率增长停滞后，处理器厂商转向了另一条路——**增加核心数量**。这就是为什么今天的 CPU 都是多核的：

| 年代 | 典型 CPU | 核心数 | 主频 |
|------|---------|--------|------|
| 2000 | Pentium 4 | 1 | 1.5 GHz |
| 2006 | Core 2 Duo | 2 | 2.4 GHz |
| 2010 | Core i7 | 4 | 3.3 GHz |
| 2017 | Core i9-7980XE | 18 | 2.6 GHz |
| 2023 | Core i9-14900K | 24 (8P+16E) | 3.2 GHz |

但多核 CPU 带来了一个新问题：**程序不会自动利用多个核心**。串行代码在 24 核 CPU 上跑，仍然只用 1 个核心。要真正利用硬件算力，必须重写代码——这就是并行编程的动机。

### 应用对算力的无尽需求

为什么一定要追求更快？因为许多真实应用的计算需求远远超出当前硬件能力：

```
计算需求 vs 硬件能力:

领域              需要的算力          意义
─────────────────────────────────────────────────────────────
实时光线追踪      ~10 TFLOPS         电影级实时渲染
蛋白质折叠        ~100 PFLOPS        理解生命的基本机制
气候模拟          ~1 EFLOPS          预测全球气候变化
LLM 训练          ~1000 EFLOPS·h     GPT-4 级别模型训练
自动驾驶感知      ~200 TOPS (INT8)   实时处理多传感器融合
─────────────────────────────────────────────────────────────

摩尔定律给的算力增长速度: ~2x / 2年
应用需求的增长速度:      远超 2x / 2年

→ 纯靠硬件进步永远追不上需求，必须靠软件并行化"榨干"硬件
```

这不仅仅是"跑得更快"的问题——**更多的算力直接转化为更好的结果**。更高的分辨率、更精确的模拟、更大的模型。并行计算是缩小算力供需差距的唯一途径。

---

## 1.2 CPU vs GPU：两种设计哲学

要理解为什么 GPU 在并行计算中如此强大，必须理解 CPU 和 GPU 根本不同的 **设计目标**。

### CPU：延迟导向（Latency-Oriented）

CPU 的设计目标是 **让单个线程跑得尽可能快**。为此，CPU 投入了大量晶体管来减少延迟：

- **大缓存（Cache）**：多级缓存（L1/L2/L3 可达数十 MB），减少内存访问延迟
- **分支预测（Branch Prediction）**：复杂的预测器提前预判分支方向，避免流水线停顿
- **乱序执行（Out-of-Order Execution）**：动态调度指令，在等待数据时执行其他无关指令
- **预取（Prefetch）**：硬件自动预取数据到缓存

这些特性让 CPU 在处理 **复杂逻辑、分支密集、串行依赖性强** 的代码时表现出色。

### GPU：吞吐导向（Throughput-Oriented）

GPU 的设计目标完全不同——**让大量线程集体完成尽可能多的工作**。GPU 采取了一种截然相反的策略：

- **小缓存**：每个计算单元配备的缓存很小
- **没有分支预测**：遇到分支就让所有路径都执行
- **没有乱序执行**：简单的顺序执行逻辑
- **海量线程 + 硬件切换**：当一组线程等待数据时，瞬间切换到另一组就绪线程

GPU 用 **大量线程的切换** 来隐藏延迟，而不是用复杂的硬件来减少延迟。

### 晶体管分配对比

这两种设计哲学的本质差异可以用 **晶体管分配** 来理解：

```
CPU 晶体管分配:
┌─────────────────────────────────────────────────┐
│                                                 │
│  ┌───────────────────────────┐  ┌──────────┐   │
│  │                           │  │          │   │
│  │    缓存 (Cache)           │  │ 控制逻辑  │   │
│  │       ~50%                │  │  ~25%    │   │
│  │                           │  │ 分支预测  │   │
│  └───────────────────────────┘  │ 乱序执行  │   │
│  ┌───────┐                      │ 预取     │   │
│  │ ALU   │ 计算单元 ~25%        └──────────┘   │
│  │ (少量) │                                     │
│  └───────┘                                     │
│                                                 │
│  目标: 单线程延迟最低                             │
└─────────────────────────────────────────────────┘

GPU 晶体管分配:
┌─────────────────────────────────────────────────┐
│                                                 │
│  ┌─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┐  │
│  │A│A│A│A│A│A│A│A│A│A│A│A│A│A│A│A│A│A│A│A│  │
│  │L│L│L│L│L│L│L│L│L│L│L│L│L│L│L│L│L│L│L│L│  │
│  │U│U│U│U│U│U│U│U│U│U│U│U│U│U│U│U│U│U│U│U│  │
│  └─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┘  │
│          计算单元 (ALU) ~80%                     │
│                                                 │
│  ┌──────┐  ┌────────┐                           │
│  │控制   │  │ 小缓存  │  ~20%                    │
│  │(简单) │  │        │                           │
│  └──────┘  └────────┘                           │
│                                                 │
│  目标: 多线程吞吐最高                             │
└─────────────────────────────────────────────────┘
```

::: tip 一个直觉的类比
CPU 像一辆法拉利——速度极快，但一次只能载几个人。GPU 像一辆公交车——单程速度慢一些，但一次能运几千人。当你需要运送几万人时，公交车的 **吞吐量** 远超法拉利。
:::

### 数据级并行（Data-Level Parallelism）

GPU 强大的根本原因在于：**许多计算密集型任务天然具有大规模数据级并行性**。

以图像处理为例：一张 4K 图像有 \(3840 \times 2160 \approx 830\) 万像素。如果要对每个像素做相同的色彩变换，这就是 830 万个独立的、完全相同的计算任务——完美适合 GPU。

类似的场景在 AI 中无处不在：

| 操作 | 并行度 | 特点 |
|------|--------|------|
| 矩阵乘法 (GEMM) | \(M \times N\) | 每个输出元素独立 |
| 卷积 | 输出通道 × 空间维度 | 每个输出像素独立 |
| 逐元素操作 (ReLU, LayerNorm) | 张量元素数 | 完全独立 |
| Attention (QK^T) | Head 数 × 序列长度 | 头间独立 |
| Embedding Lookup | Batch × 序列长度 | 每个 token 独立 |

**GPU 的计算能力远超 CPU**。以 2022 年的硬件为例：

| 指标 | Intel i9-13900K (CPU) | NVIDIA H100 (GPU) | GPU/CPU |
|------|----------------------|-------------------|---------|
| FP32 峰值 | ~1 TFLOPS | ~60 TFLOPS | **60x** |
| FP16 峰值 | — | ~990 TFLOPS (Tensor Core) | — |
| 内存带宽 | ~90 GB/s (DDR5) | ~3.35 TB/s (HBM3) | **37x** |
| 核心数 | 24 (8P+16E) | 16,896 CUDA Cores | — |
| TDP | 253 W | 700 W | 2.8x |

GPU 用约 2.8 倍的功耗，换来了 60 倍的计算能力和 37 倍的内存带宽——**能效比远超 CPU**。

---

## 1.3 异构计算：CPU + GPU 协作

虽然 GPU 的并行计算能力碾压 CPU，但 **GPU 并不能取代 CPU**。原因很简单：现实世界的程序不可能 100% 并行化。

### Amdahl's Law：并行加速的上限

Amdahl 定律指出，假设程序中有 \(p\) 比例的部分可以被完美并行化，则整体加速比的上限为：

$$
S = \frac{1}{(1 - p) + \frac{p}{N}}
$$

其中 \(N\) 是并行处理器数量。当 \(N \to \infty\) 时：

$$
S_{\max} = \frac{1}{1 - p}
$$

```
Amdahl's Law: 加速比 vs 并行化比例

加速比 S
20x ┤
    │                                              ●  p=95%
    │
15x ┤
    │
    │                                 ●  p=90%
10x ┤
    │
    │                    ●  p=80%
 5x ┤
    │
    │       ●  p=50%
 2x ┤  ●  p=30%
 1x ┼────┬────┬────┬────┬────┬────┬────┬────┬────┬──
   1    2    4    8   16   32   64  128  256  512  ∞
                     处理器数量 N

即使有无限个处理器:
  p = 50% → 最多 2x 加速
  p = 90% → 最多 10x 加速
  p = 99% → 最多 100x 加速

→ 串行部分决定了加速上限!
```

这意味着：

- 如果程序只有 50% 可以并行化，即使有无限 GPU 核心，加速也不会超过 **2 倍**
- 即使 99% 可以并行化，上限也只是 **100 倍**
- **串行部分是无法被 GPU 加速的**，必须由 CPU 高效执行

### 异构计算模型

这就是为什么现代计算采用 **异构（Heterogeneous）** 模型——CPU 和 GPU 各司其职：

```
异构计算模型:

                    程序执行流程
                ┌──────────────────┐
                │   串行代码段      │ ← CPU 执行
                │  (初始化、I/O、   │    (分支、逻辑、串行依赖)
                │   控制流逻辑)     │
                └────────┬─────────┘
                         │
           ┌─────────────┼─────────────┐
           ▼             ▼             ▼
    ┌────────────┐┌────────────┐┌────────────┐
    │ GPU 并行   ││ GPU 并行   ││ GPU 并行   │ ← GPU 执行
    │ 任务 1     ││ 任务 2     ││ 任务 3     │    (大规模数据并行)
    │ (GEMM)     ││ (Conv)     ││ (Attention)│
    └────────────┘└────────────┘└────────────┘
           │             │             │
           └─────────────┼─────────────┘
                         │
                ┌────────┴─────────┐
                │   串行代码段      │ ← CPU 执行
                │  (结果后处理、    │
                │   下一步决策)     │
                └──────────────────┘

原则: 串行归 CPU, 并行归 GPU, 各取所长
```

在 CUDA 的术语中：
- CPU 称为 **Host（主机）**
- GPU 称为 **Device（设备）**
- 在 GPU 上执行的函数称为 **Kernel（核函数）**

程序的执行流程是 Host 代码和 Device Kernel 交替执行。Host 负责数据准备、Kernel 启动和结果回收；Device 负责大规模并行计算。

---

## 1.4 GPU 发展简史：从图形渲染到通用计算

GPU 最初是为了加速图形渲染而设计的，后来逐步演化为通用并行处理器。理解这段历史有助于理解 GPU 架构为什么长成今天这个样子。

### 图形管线时代（~2000 年前）

早期 GPU 是 **固定功能管线（Fixed-Function Pipeline）**——硬件电路固化了图形渲染的各个阶段（顶点变换、光栅化、纹理采样、像素着色等），程序员只能通过 API（OpenGL/DirectX）配置参数，无法编写自定义程序。

```
固定功能图形管线:

顶点数据 → [顶点变换] → [裁剪] → [光栅化] → [纹理采样] → [像素混合] → 帧缓冲
              固定硬件    固定硬件   固定硬件    固定硬件      固定硬件

特点: 高效但完全不灵活, 只能做图形
```

### 可编程着色器时代（2001-2006）

DirectX 8/9 引入了 **可编程着色器（Programmable Shaders）**，允许程序员编写自定义的顶点着色器和像素着色器。一些有创意的研究者发现，可以通过"伪装"数据为纹理、计算为着色器来做通用计算——这就是 **GPGPU（General-Purpose computing on GPU）** 的雏形。

```
GPGPU 时代的 "黑魔法":

要计算: C = A + B (两个数组相加)

实际操作:
  1. 把数组 A 和 B 编码为纹理 (Texture)
  2. 画一个全屏四边形 (触发像素着色器)
  3. 在像素着色器中: 采样纹理 A 和 B, 相加
  4. 输出到渲染目标 (Render Target)
  5. 把渲染目标读回来 → 这就是 C

问题: 极度不直觉、难调试、受图形 API 限制
```

### CUDA 时代（2006 至今）

2006 年，NVIDIA 推出了 **CUDA（Compute Unified Device Architecture）**，这是 GPU 计算的分水岭。CUDA 做了几件关键的事：

1. **统一着色器架构**：不再区分顶点着色器和像素着色器，所有计算单元统一为通用处理器
2. **C 语言扩展**：程序员可以用类似 C 的语言直接编写 GPU 程序，不再需要图形 API
3. **开放内存访问**：允许对 GPU 内存的任意读写（scatter/gather），而非受限于图形管线的固定模式
4. **软件生态**：提供完整的编译器（nvcc）、调试器（cuda-gdb）、性能分析器（Nsight/nvprof）工具链

```
GPU 计算范式的演进:

 固定功能          可编程着色器          CUDA / 通用计算
(~2000前)        (2001-2006)         (2006至今)
                                     
┌──────────┐    ┌──────────┐        ┌──────────┐
│ 顶点处理  │    │ VS  │ PS  │       │          │
│ 像素处理  │    │(可编程)   │       │ 通用SM    │
│ (固定硬件)│    │          │       │ (CUDA     │
│          │    │ 受图形API │       │  Cores)  │
│          │    │ 约束      │       │          │
└──────────┘    └──────────┘        │ 完全可编程│
                                    │ C/C++ 扩展│
不灵活         灵活但别扭            └──────────┘
只能做图形     需要"伪装"            直接编程
                为图形操作            自由灵活
```

### NVIDIA GPU 架构演进

CUDA 发布后，NVIDIA 每 1-2 年推出一代新的 GPU 架构，每代都带来重大的硬件改进：

| 架构 | 年份 | 代表型号 | 关键特性 |
|------|------|---------|---------|
| Tesla | 2006 | 8800 GTX | CUDA 诞生，统一着色器 |
| Fermi | 2010 | GTX 480 | L1/L2 Cache，ECC，双精度大幅提升 |
| Kepler | 2012 | K40 | Dynamic Parallelism，Hyper-Q |
| Maxwell | 2014 | GTX 980 | 能效革命，每瓦性能翻倍 |
| Pascal | 2016 | P100/GTX 1080 | HBM2，NVLink，统一内存增强 |
| Volta | 2017 | V100 | **Tensor Core** 诞生，独立 FP/INT 管线 |
| Turing | 2018 | RTX 2080 | RT Core (光追)，INT8/INT4 推理 |
| Ampere | 2020 | A100 | TF32，稀疏 Tensor Core，MIG |
| Hopper | 2022 | H100 | FP8，Transformer Engine，DPX |
| Blackwell | 2024 | B200 | 第二代 Transformer Engine，FP4 |

::: tip 关注 AI Infra 的人
如果你做 LLM 推理/训练优化，最需要关注的架构跳跃是 **Volta（V100）→ Ampere（A100）→ Hopper（H100）**。Tensor Core 的引入（Volta）和持续增强（Ampere 的稀疏支持、Hopper 的 FP8）是 AI 算力爆炸式增长的硬件基础。
:::

---

## 1.5 CUDA 编程模型概览

本节不深入代码细节（第二章会展开），而是建立对 CUDA 编程模型的整体认知框架。

### Host-Device 执行模型

CUDA 程序的执行流程可以概括为三步：

```
CUDA 程序基本流程:

     Host (CPU)                        Device (GPU)
┌──────────────────┐              ┌──────────────────┐
│                  │              │                  │
│ 1. 分配 GPU 内存 │ ──malloc──►  │   [GPU Memory]   │
│                  │              │                  │
│ 2. 拷贝数据到 GPU│ ──memcpy──►  │   [Input Data]   │
│                  │              │                  │
│ 3. 启动 Kernel   │ ──launch──►  │   ┌──────────┐   │
│                  │              │   │ Kernel    │   │
│    (CPU 继续     │              │   │ 并行执行   │   │
│     做其他事)    │              │   │ 数千线程   │   │
│                  │              │   └──────────┘   │
│ 4. 拷贝结果回 CPU│ ◄──memcpy──  │   [Output Data]  │
│                  │              │                  │
│ 5. 释放 GPU 内存 │ ──free────►  │                  │
│                  │              │                  │
└──────────────────┘              └──────────────────┘
```

对应的代码骨架：

```c
// CUDA 程序的基本骨架
void gpu_computation(float* h_input, float* h_output, int n) {
    float *d_input, *d_output;

    // 1. 分配 GPU 内存
    cudaMalloc(&d_input, n * sizeof(float));
    cudaMalloc(&d_output, n * sizeof(float));

    // 2. 拷贝输入数据到 GPU
    cudaMemcpy(d_input, h_input, n * sizeof(float), cudaMemcpyHostToDevice);

    // 3. 启动 Kernel (在 GPU 上并行执行)
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;
    myKernel<<<gridSize, blockSize>>>(d_input, d_output, n);

    // 4. 拷贝结果回 CPU
    cudaMemcpy(h_output, d_output, n * sizeof(float), cudaMemcpyDeviceToHost);

    // 5. 释放 GPU 内存
    cudaFree(d_input);
    cudaFree(d_output);
}
```

### 线程层次结构

CUDA 最核心的抽象是 **线程层次结构（Thread Hierarchy）**，这也是整本书反复展开的主题：

```
CUDA 线程层次结构:

Grid (网格)
┌─────────────────────────────────────────────────────┐
│                                                     │
│  Block(0,0)      Block(1,0)      Block(2,0)        │
│  ┌───────────┐  ┌───────────┐  ┌───────────┐      │
│  │ t0 t1 t2  │  │ t0 t1 t2  │  │ t0 t1 t2  │      │
│  │ t3 t4 t5  │  │ t3 t4 t5  │  │ t3 t4 t5  │      │
│  │ t6 t7 t8  │  │ t6 t7 t8  │  │ t6 t7 t8  │      │
│  └───────────┘  └───────────┘  └───────────┘      │
│                                                     │
│  Block(0,1)      Block(1,1)      Block(2,1)        │
│  ┌───────────┐  ┌───────────┐  ┌───────────┐      │
│  │ t0 t1 t2  │  │ t0 t1 t2  │  │ t0 t1 t2  │      │
│  │ t3 t4 t5  │  │ t3 t4 t5  │  │ t3 t4 t5  │      │
│  │ t6 t7 t8  │  │ t6 t7 t8  │  │ t6 t7 t8  │      │
│  └───────────┘  └───────────┘  └───────────┘      │
│                                                     │
│  gridDim = (3, 2)   blockDim = (3, 3)              │
│  总线程数 = 3×2 × 3×3 = 54                         │
└─────────────────────────────────────────────────────┘

层次:
  Grid   → 一个 Kernel 启动产生一个 Grid
  Block  → Grid 由多个 Block 组成, Block 间独立
  Thread → Block 由多个 Thread 组成, Thread 间可同步
  Warp   → 32 个连续 Thread 组成一个 Warp (硬件调度单位)
```

每个线程通过内置变量知道自己的"身份"：

| 变量 | 含义 |
|------|------|
| `threadIdx.x/y/z` | 线程在 Block 内的坐标 |
| `blockIdx.x/y/z` | Block 在 Grid 内的坐标 |
| `blockDim.x/y/z` | 每个 Block 的维度大小 |
| `gridDim.x/y/z` | Grid 的维度大小 |

全局线程 ID 的典型计算方式（1D）：

$$
\text{globalIdx} = \texttt{blockIdx.x} \times \texttt{blockDim.x} + \texttt{threadIdx.x}
$$

### 内存层次结构预览

GPU 的内存层次结构是性能优化的核心（第 5、6 章会深入），这里给一个概览：

```
GPU 内存层次结构:

┌─────────────────────────────────────────────────┐
│                  Off-Chip (片外)                  │
│  ┌─────────────────────────────────────────────┐ │
│  │        Global Memory / HBM                  │ │
│  │        容量: 数十 GB    带宽: ~TB/s          │ │
│  │        延迟: ~数百 cycles                    │ │
│  │        所有线程均可访问                        │ │
│  └─────────────────────────────────────────────┘ │
│                       ↑↓                         │
├─────────────────────────────────────────────────┤
│                  On-Chip (片上)                   │
│                                                 │
│  ┌─────────────────┐  ┌──────────────────────┐  │
│  │  L2 Cache       │  │  Constant Memory     │  │
│  │  数 MB          │  │  (只读, 带缓存)       │  │
│  └────────┬────────┘  └──────────────────────┘  │
│           ↑↓                                    │
│  ┌──────────────────────────────────────────┐   │
│  │  Per-SM Resources                        │   │
│  │  ┌──────────────┐ ┌───────────────────┐  │   │
│  │  │Shared Memory │ │ L1 Cache          │  │   │
│  │  │(可编程)       │ │(硬件管理)          │  │   │
│  │  │ ~100 KB/SM   │ │                   │  │   │
│  │  │ 延迟: ~数 cycles│                   │  │   │
│  │  │ Block 内共享   │ │                   │  │   │
│  │  └──────────────┘ └───────────────────┘  │   │
│  │  ┌──────────────┐                        │   │
│  │  │ Registers    │ Thread 私有             │   │
│  │  │ ~256KB/SM    │ 延迟: 0 cycles         │   │
│  │  └──────────────┘                        │   │
│  └──────────────────────────────────────────┘   │
└─────────────────────────────────────────────────┘

速度: Registers > Shared Memory > L2 > Global Memory
容量: Registers < Shared Memory < L2 < Global Memory
```

性能优化的核心就是：**让数据尽量待在快的地方，减少对慢内存的访问**。这个主题会贯穿全书。

---

## 1.6 并行编程的挑战

GPU 并行编程不是把循环扔给 GPU 就完事了。书中点出了几个核心挑战：

### 1. 算法设计：如何分解问题？

并非所有算法都能自然并行化。有些问题天然并行（如逐像素处理），有些问题存在复杂的数据依赖（如递推关系、图的 BFS）。

本书后半部分（Ch 7-18）的价值就在于：展示了 **如何把看似串行的问题转化为并行算法**——卷积、归约、前缀和、排序、稀疏矩阵运算、图遍历，每一个都是经典的并行设计模式。

### 2. 性能优化：瓶颈在哪里？

GPU 的峰值算力很高，但实际能用多少取决于：

- **内存带宽利用率**：数据能不能快速送达计算单元？（Coalescing, Tiling）
- **计算单元利用率**：是不是所有线程都在干活？（Occupancy, Divergence）
- **指令级并行度**：单线程内有没有足够的独立指令？（ILP）

这三者相互交织，形成了 GPU 性能优化的"铁三角"。Ch 4-6 会深入讨论。

### 3. 正确性：并发下的数据竞争

多个线程同时访问共享数据时，如果缺乏适当的同步机制，就会产生 **竞态条件（Race Condition）**——程序的结果取决于线程执行的随机顺序。

CUDA 提供了几种同步机制：
- **`__syncthreads()`**：Block 内所有线程的屏障同步
- **原子操作（Atomic Operations）**：对共享内存/全局内存的原子读-改-写
- **Cooperative Groups**：更灵活的线程分组同步（第四版新增）

### 4. 可移植性与可维护性

GPU 架构在不断演进（每代 SM 数量、Shared Memory 大小、Warp 调度策略都可能变化）。好的 CUDA 代码应该在不同架构上都能高效运行，而不是过度针对某一代硬件优化。

---

## 1.7 本章总结与后续预览

### 核心要点回顾

| 要点 | 内容 |
|------|------|
| **为什么需要并行** | 频率增长停滞（功耗墙），只能通过并行提升性能 |
| **CPU vs GPU** | CPU 延迟导向（单线程快），GPU 吞吐导向（多线程总量大） |
| **异构计算** | CPU 处理串行逻辑，GPU 处理大规模并行计算，各取所长 |
| **Amdahl's Law** | 并行加速受限于串行部分比例 |
| **GPU 演进** | 固定管线 → 可编程着色器 → CUDA 通用计算 |
| **线程层次** | Grid → Block → Thread → Warp |
| **内存层次** | Register → Shared Memory → L2 Cache → Global Memory |
| **核心挑战** | 算法设计、性能优化、正确性、可移植性 |

### 后续章节预览

```
本书的知识地图:

Ch 1 (本章)              为什么需要 GPU 并行计算？
      │
Ch 2-3                   CUDA 基础: 第一个程序 + 多维数据映射
      │
Ch 4-6  ──────────────── 核心: GPU 架构 + 内存优化 + 性能分析
      │                  (这三章是全书精华)
      │
Ch 7-15 ──────────────── 并行算法模式:
      │                  卷积、Stencil、直方图、归约、
      │                  前缀和、归并、排序、稀疏矩阵、图
      │
Ch 16-18 ─────────────── 应用案例:
      │                  深度学习、MRI 重建、分子动力学
      │
Ch 19-22 ─────────────── 高级主题:
                         计算思维、集群编程、动态并行、未来展望
```

::: tip 下一章预告
[Ch02: Heterogeneous Data Parallel Computing](./ch02) 将带你写出第一个 CUDA 程序——向量加法。你会学到 `cudaMalloc`、`cudaMemcpy`、Kernel 启动语法 `<<<grid, block>>>` 等基础 API，并理解 Host-Device 交互的完整流程。
:::

---

## 扩展思考

::: details 思考题 1：GPU 的吞吐导向设计在哪些场景下反而是劣势？
GPU 牺牲了单线程性能来换取吞吐量。在以下场景中 GPU 可能不如 CPU：

1. **分支密集的控制流代码**：if-else 嵌套很深、switch-case 分支很多的代码，会导致 Warp Divergence，GPU 效率急剧下降
2. **串行依赖性强的递推计算**：如斐波那契数列 `f(n) = f(n-1) + f(n-2)`，每步依赖上一步结果，无法并行
3. **低并行度任务**：如果只有几十个独立任务，GPU 的数千核心大部分闲置，启动 Kernel 的开销反而使性能更差
4. **频繁的 Host-Device 数据传输**：如果计算量小但需要频繁在 CPU 和 GPU 之间搬数据，PCIe 传输延迟会成为瓶颈
:::

::: details 思考题 2：为什么 CUDA 选择 C/C++ 作为基础语言，而不是设计全新语言？
这是一个非常务实的工程决策：

1. **降低学习曲线**：大量 HPC 和图形开发者已经熟悉 C/C++，在此基础上扩展（添加 `__global__`、`<<<>>>` 等语法）比学一门新语言门槛低得多
2. **复用现有生态**：C/C++ 的标准库、第三方库、构建系统都可以直接使用
3. **性能可控**：C/C++ 给程序员对内存布局、指针操作的完全控制，这在 GPU 编程中至关重要
4. **编译器实现简便**：nvcc 实际上是预处理器 + 主机编译器的组合，Host 代码直接交给 GCC/MSVC/Clang，Device 代码由 NVIDIA 编译器处理

后来 NVIDIA 也支持了 Fortran (CUDA Fortran) 和 Python (Numba CUDA, PyCUDA)，但 C/C++ 仍然是性能最优和使用最广的选择。
:::

::: details 思考题 3：Amdahl's Law 是否意味着并行计算的收益是有限的？
Amdahl's Law 给出的是悲观上限，实际中有几点缓解因素：

1. **Gustafson's Law**：Amdahl 假设问题规模固定。但实际中，当有更多处理器时，人们通常会增大问题规模（更大的 batch size、更高的分辨率），而不是固守原始规模。Gustafson's Law 给出的视角是：固定执行时间，更多处理器 → 解决更大的问题
2. **串行部分可以被优化**：很多"看似串行"的代码其实可以被并行化，比如本书后续章节会展示归约、前缀和等"看起来串行"的操作如何高效并行化
3. **流水线重叠**：通过 CUDA Streams 和异步执行，Host-Device 传输可以与计算重叠，实际的串行等待时间远小于理论值
:::
