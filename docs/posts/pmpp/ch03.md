---
title: "PMPP Ch03: Multidimensional Grids and Data"
date: 2026-02-19
---

# Ch03: Multidimensional Grids and Data

<p style="color: var(--vp-c-text-2); font-size: 14px;">
2026-02-19 &nbsp;·&nbsp; PMPP 专栏 &nbsp;·&nbsp; 第三章
</p>

> **本章信息**
> - **书名**: Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)
> - **作者**: David B. Kirk, Wen-mei W. Hwu
> - **范围**: 第 3 章 Multidimensional Grids and Data
> - **配套代码**: [ch03_color2gray.cu](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch03_color2gray.cu) · [ch03_matmul.cu](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch03_matmul.cu)

## 一句话总结

本章将第二章的 1D 向量加法推广到 **多维** 场景——图像处理和矩阵运算。核心内容是如何使用 **2D/3D 的 Grid 和 Block** 来映射多维数据，以及理解 C/CUDA 中多维数组的 **行优先（Row-Major）内存布局**。这是编写图像处理、矩阵运算等 Kernel 的基础。

---

## 3.1 多维 Grid 与 Block：`dim3` 类型

第二章中我们用标量配置 Grid 和 Block（`<<<gridSize, blockSize>>>`），这只适合 1D 数据。对于 2D 图像和矩阵，CUDA 提供了 `dim3` 类型来指定多维网格：

```c
// 1D 配置 (第二章)
int blockSize = 256;
int gridSize = (n + blockSize - 1) / blockSize;
kernel<<<gridSize, blockSize>>>(...)

// 2D 配置 (本章)
dim3 blockDim(16, 16);       // 每 Block: 16×16 = 256 线程
dim3 gridDim(
    (width  + 15) / 16,      // x 方向: 覆盖列数
    (height + 15) / 16       // y 方向: 覆盖行数
);
kernel<<<gridDim, blockDim>>>(...)
```

`dim3` 是 CUDA 内置的三维整数向量类型，未指定的维度默认为 1：

```c
dim3 a(4);        // (4, 1, 1) — 等价于 1D
dim3 b(4, 8);     // (4, 8, 1) — 2D
dim3 c(4, 8, 2);  // (4, 8, 2) — 3D
```

### 2D Grid/Block 的线程组织

```
2D Grid (gridDim = (3, 2)) + 2D Block (blockDim = (4, 4)):

gridDim.y = 2
│
│  Block(0,0)       Block(1,0)       Block(2,0)
│  ┌──┬──┬──┬──┐   ┌──┬──┬──┬──┐   ┌──┬──┬──┬──┐
│  │00│10│20│30│   │00│10│20│30│   │00│10│20│30│  threadIdx
│  │01│11│21│31│   │01│11│21│31│   │01│11│21│31│  (x, y)
│  │02│12│22│32│   │02│12│22│32│   │02│12│22│32│
│  │03│13│23│33│   │03│13│23│33│   │03│13│23│33│
│  └──┴──┴──┴──┘   └──┴──┴──┴──┘   └──┴──┴──┴──┘
│
│  Block(0,1)       Block(1,1)       Block(2,1)
│  ┌──┬──┬──┬──┐   ┌──┬──┬──┬──┐   ┌──┬──┬──┬──┐
│  │00│10│20│30│   │00│10│20│30│   │00│10│20│30│
│  │01│11│21│31│   │01│11│21│31│   │01│11│21│31│
│  │02│12│22│32│   │02│12│22│32│   │02│12│22│32│
│  │03│13│23│33│   │03│13│23│33│   │03│13│23│33│
│  └──┴──┴──┴──┘   └──┴──┴──┴──┘   └──┴──┴──┴──┘
│
└───────── gridDim.x = 3 ─────────►
```

每个线程的全局 2D 坐标：

$$
\text{col} = \texttt{blockIdx.x} \times \texttt{blockDim.x} + \texttt{threadIdx.x}
$$

$$
\text{row} = \texttt{blockIdx.y} \times \texttt{blockDim.y} + \texttt{threadIdx.y}
$$

::: tip x 对应列 (col)，y 对应行 (row)
这是一个容易搞混的点。CUDA 的 `dim3` 中 `.x` 是最内层维度，映射到数据的 **列**；`.y` 是第二层维度，映射到 **行**。这与数学中矩阵 \(M_{row, col}\) 的习惯（先行后列）是 **反的**，需要特别注意。
:::

---

## 3.2 行优先内存布局（Row-Major Layout）

### 多维数组在内存中是一维的

无论数据逻辑上是 2D 还是 3D，**物理内存是一维连续的**。C/C++ 和 CUDA 使用 **行优先布局（Row-Major）**：同一行的元素在内存中相邻，不同行之间依次排列。

```
逻辑视图 (3×4 矩阵):            物理内存 (行优先):

       col 0  col 1  col 2  col 3
row 0 [  0      1      2      3  ]    [ 0  1  2  3 | 4  5  6  7 | 8  9  10  11 ]
row 1 [  4      5      6      7  ]      row 0         row 1         row 2
row 2 [  8      9     10     11  ]
                                       ← 地址递增方向
```

### 索引计算公式

**2D → 1D 线性化**：

$$
\text{index} = \text{row} \times \text{width} + \text{col}
$$

**3D → 1D 线性化**（如 RGB 图像 / 视频帧）：

$$
\text{index} = \text{depth} \times (\text{height} \times \text{width}) + \text{row} \times \text{width} + \text{col}
$$

通用规则：**最右边的维度变化最快，在内存中最连续**。

```
行优先 vs 列优先:

行优先 (Row-Major, C/C++/CUDA/Python-numpy默认):
  M[i][j] 存储在 address = base + i * num_cols + j
  同一行的元素内存连续: M[0][0], M[0][1], M[0][2], ...

列优先 (Column-Major, Fortran/MATLAB/Julia):
  M[i][j] 存储在 address = base + j * num_rows + i
  同一列的元素内存连续: M[0][0], M[1][0], M[2][0], ...
```

::: warning 行优先 vs 列优先的性能影响
GPU 的全局内存访问以 **128 字节** 为单位（一个 cache line / memory transaction）。如果一个 Warp 中的 32 个线程访问同一行中连续的 32 个 float（128 字节），则只需 **1 次** 内存事务（Memory Transaction）。如果 32 个线程访问的是同一列的 32 个元素（每个相隔 width 个 float），则需要多达 **32 次** 内存事务——慢了 32 倍。

这就是 **内存合并访问（Memory Coalescing）** 的核心概念，第 6 章会深入展开。此处只需记住：**让相邻线程访问相邻内存地址**。
:::

---

## 3.3 案例一：彩色图像转灰度

### 问题描述

将 RGB 彩色图像转为灰度图。每个像素有 R、G、B 三个通道，灰度值的标准加权公式（ITU-R BT.601）为：

$$
\text{Gray} = 0.21 \times R + 0.72 \times G + 0.07 \times B
$$

这三个权重反映了人眼对不同颜色的敏感度——对绿色最敏感（0.72），蓝色最不敏感（0.07）。

### 数据布局

RGB 图像在内存中有两种常见布局：

```
交错布局 (Interleaved / AoS - Array of Structures):
  [R₀ G₀ B₀ | R₁ G₁ B₁ | R₂ G₂ B₂ | ...]
  每个像素的 RGB 三通道紧邻存储
  → 这是 BMP/PNG 等图像格式的默认布局

平面布局 (Planar / SoA - Structure of Arrays):
  [R₀ R₁ R₂ ... | G₀ G₁ G₂ ... | B₀ B₁ B₂ ...]
  同一通道的所有像素连续存储
  → GPU 友好 (更好的合并访问), 常用于深度学习框架
```

本例使用交错布局（与实际图像文件一致）。

### CPU 实现

```c
void color2gray_cpu(const unsigned char* rgb, unsigned char* gray,
                    int width, int height) {
    for (int row = 0; row < height; row++) {
        for (int col = 0; col < width; col++) {
            int idx = row * width + col;
            unsigned char r = rgb[3 * idx + 0];
            unsigned char g = rgb[3 * idx + 1];
            unsigned char b = rgb[3 * idx + 2];
            gray[idx] = (unsigned char)(0.21f * r + 0.72f * g + 0.07f * b);
        }
    }
}
```

### CUDA Kernel

```c
__global__ void color2gray_kernel(const unsigned char* rgb,
                                   unsigned char* gray,
                                   int width, int height) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;

    if (col < width && row < height) {
        int idx = row * width + col;
        unsigned char r = rgb[3 * idx + 0];
        unsigned char g = rgb[3 * idx + 1];
        unsigned char b = rgb[3 * idx + 2];
        gray[idx] = (unsigned char)(0.21f * r + 0.72f * g + 0.07f * b);
    }
}
```

**从 CPU 到 GPU 的转换**：

```
CPU (双重循环):                    GPU (2D 线程映射):

for (row = 0; row < H; row++)      col = blockIdx.x * blockDim.x + threadIdx.x
  for (col = 0; col < W; col++)    row = blockIdx.y * blockDim.y + threadIdx.y
  {                                if (col < W && row < H) {
    idx = row * W + col;             idx = row * W + col;
    gray[idx] = 0.21*R+...          gray[idx] = 0.21*R+...
  }                                }

两层循环 → 2D 线程 ID
循环变量 row, col → blockIdx/threadIdx 计算
无需边界检查 → 需要 if 检查 (Block 可能超出图像边界)
```

### Kernel 启动配置

```c
dim3 blockDim(16, 16);   // 16×16 = 256 threads
dim3 gridDim((width  + 15) / 16,
             (height + 15) / 16);

color2gray_kernel<<<gridDim, blockDim>>>(d_rgb, d_gray, width, height);
```

为什么 Block 用 16×16 而不是 256×1？

```
一张 1920×1080 的图像:

方案 A: blockDim = (256, 1) — 1D 布局
  gridDim = (ceil(1920/256), 1080) = (8, 1080) = 8640 个 Block
  每个 Block 覆盖一行中的 256 像素
  → 可以工作, 但不太自然

方案 B: blockDim = (16, 16) — 2D 布局
  gridDim = (ceil(1920/16), ceil(1080/16)) = (120, 68) = 8160 个 Block
  每个 Block 覆盖 16×16 = 一个像素方块
  → 更自然, 且对 2D 空间局部性更友好
     (未来做图像模糊等需要邻域操作时, 2D Block 优势明显)
```

::: tip 16×16 的来源
\(16 \times 16 = 256\)，既是 32（Warp 大小）的整数倍，又是一个不太大的正方形。这使得 2D 数据映射均匀，同时保证每个 Block 的线程数在一个合理范围内。在图像处理和矩阵运算中，16×16 或 32×32 的 Block 是最常见的选择。
:::

---

## 3.4 案例二：矩阵乘法（基础版本）

### 问题描述

计算矩阵乘法 \(C = A \times B\)，其中 \(A\) 是 \(M \times K\)，\(B\) 是 \(K \times N\)，结果 \(C\) 是 \(M \times N\)。

$$
C[i][j] = \sum_{k=0}^{K-1} A[i][k] \cdot B[k][j]
$$

每个输出元素 \(C[i][j]\) 是 \(A\) 的第 \(i\) 行与 \(B\) 的第 \(j\) 列的点积——这是一个天然的数据并行问题：\(M \times N\) 个输出元素可以 **完全独立** 计算。

### 内存访问模式分析

```
矩阵乘法 C[row][col] = Σ A[row][k] * B[k][col]:

矩阵 A (M×K)              矩阵 B (K×N)              矩阵 C (M×N)
┌──────────────┐          ┌──────────────┐          ┌──────────────┐
│→→→→→→→→→→→→→│ row      │  ↓           │          │              │
│              │          │  ↓           │          │         ●    │ C[row][col]
│              │          │  ↓   col     │          │              │
└──────────────┘          └──────────────┘          └──────────────┘

计算 C[row][col]:
  → 遍历 A 的第 row 行 (K 个元素) — 内存连续 ✓
  → 遍历 B 的第 col 列 (K 个元素) — 内存不连续 ✗ (每个元素间隔 N)
  → 做 K 次乘法和 K 次加法
```

### CPU 实现

```c
void matmul_cpu(const float* A, const float* B, float* C,
                int M, int K, int N) {
    for (int row = 0; row < M; row++) {
        for (int col = 0; col < N; col++) {
            float sum = 0.0f;
            for (int k = 0; k < K; k++) {
                sum += A[row * K + k] * B[k * N + col];
            }
            C[row * N + col] = sum;
        }
    }
}
```

注意索引计算：
- `A[row * K + k]`：A 有 K 列，所以行偏移是 `row * K`
- `B[k * N + col]`：B 有 N 列，所以行偏移是 `k * N`
- `C[row * N + col]`：C 有 N 列

### CUDA Kernel

```c
__global__ void matmul_kernel(const float* A, const float* B, float* C,
                               int M, int K, int N) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;

    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += A[row * K + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}
```

**关键观察**：每个线程内部仍然有一个 `for` 循环（遍历 K 维度做点积）。并行化的是 **外层两重循环**（遍历 M×N 个输出元素），第三重循环（K 维度的累加）在每个线程内串行执行。

```
矩阵乘法的并行结构:

    CPU: 三重循环                  GPU: 一层循环 × M×N 个线程

    for row = 0..M-1:              Thread (row, col):
      for col = 0..N-1:  ← 并行化 →   for k = 0..K-1:     ← 每线程内串行
        for k = 0..K-1:                  sum += A[row][k] * B[k][col]
          sum += ...                  C[row][col] = sum
```

### 线程到数据的映射

```
2D Grid 映射到矩阵 C (M=8, N=12, blockDim = 4×4):

          col →
          0   1   2   3   4   5   6   7   8   9  10  11
        ┌───────────────┬───────────────┬───────────────┐
  row 0 │               │               │               │
      1 │  Block(0,0)   │  Block(1,0)   │  Block(2,0)   │
      2 │               │               │               │
      3 │               │               │               │
   ↓  ├───────────────┼───────────────┼───────────────┤
      4 │               │               │               │
      5 │  Block(0,1)   │  Block(1,1)   │  Block(2,1)   │
      6 │               │               │               │
      7 │               │               │               │
        └───────────────┴───────────────┴───────────────┘

gridDim  = (ceil(12/4), ceil(8/4)) = (3, 2)

Block(1,0) 中的 Thread(2, 3):
  col = 1 * 4 + 2 = 6
  row = 0 * 4 + 3 = 3
  → 计算 C[3][6]
```

### 性能分析：这个基础版本有多慢？

虽然这个 Kernel 是正确的，但它的性能 **很差**。让我们分析原因：

**每个线程的全局内存访问**：
- 读取 A 的一行：K 次读取
- 读取 B 的一列：K 次读取
- 写入 C 的一个元素：1 次写入
- 总计：\(2K + 1\) 次全局内存访问

**所有线程的总全局内存访问**：
- 计算 C 的 M×N 个元素，每个需要 \(2K + 1\) 次
- 总计：\(M \times N \times (2K + 1)\) 次

但实际上 A 和 B 加起来只有 \(MK + KN\) 个元素。每个 A 的元素被读取了 \(N\) 次（同一行的不同线程），每个 B 的元素被读取了 \(M\) 次（同一列的不同线程）——**巨大的冗余全局内存访问**。

```
全局内存访问冗余分析 (M=N=K=1024):

                      A 的元素     B 的元素
实际数据量:            1M           1M          = 2M 个 float (8 MB)
每个元素被读取次数:    N = 1024     M = 1024
总读取量:              1G           1G          = 2G 个 float (8 GB)
冗余倍数:              1024x        1024x

全局内存带宽 (A100): 2039 GB/s
需要读取: 8 GB
理论 Kernel 耗时 (带宽受限): 8 / 2039 ≈ 3.9 ms

实际计算量: 2 × 1024³ ≈ 2.1 GFLOP
如果跑到峰值 19.5 TFLOPS: 2.1 / 19500 ≈ 0.11 ms

→ 带宽瓶颈 (3.9 ms) 远大于计算瓶颈 (0.11 ms)
→ 解决方案: Shared Memory Tiling (第 5 章)
```

```cpp-run title="矩阵乘法全局内存访问分析"
#include <iostream>
#include <iomanip>
#include <cmath>

int main() {
    std::cout << "=== 基础矩阵乘法: 全局内存访问分析 ===\n\n";

    std::cout << std::setw(8) << "M=N=K"
              << std::setw(14) << "数据量(MB)"
              << std::setw(14) << "实际读(GB)"
              << std::setw(14) << "冗余倍数"
              << std::setw(14) << "计算(GFLOP)"
              << std::setw(14) << "AI(FLOP/B)" << "\n";
    std::cout << std::string(78, '-') << "\n";

    for (int n : {256, 512, 1024, 2048, 4096}) {
        long long M = n, K = n, N = n;
        double data_mb = (M*K + K*N) * 4.0 / 1e6;
        double total_reads = (double)M * N * (2.0 * K) * 4.0 / 1e9;
        double redundancy = (double)M * N * 2.0 * K / (M*K + K*N);
        double gflops = 2.0 * M * N * K / 1e9;
        double ai = 2.0 * M * N * K / ((double)M * N * (2.0*K+1) * 4.0);

        std::cout << std::setw(8) << n
                  << std::setw(11) << std::fixed << std::setprecision(1)
                  << data_mb << " MB"
                  << std::setw(11) << std::setprecision(1)
                  << total_reads << " GB"
                  << std::setw(11) << std::setprecision(0)
                  << redundancy << "x"
                  << std::setw(11) << std::setprecision(1)
                  << gflops
                  << std::setw(14) << std::setprecision(2)
                  << ai << "\n";
    }

    std::cout << "\n";
    std::cout << "AI (Arithmetic Intensity) = FLOP / 实际访存字节数\n";
    std::cout << "  → 基础版本的 AI ≈ 0.25, 远低于 GPU 的 Roofline 拐点\n";
    std::cout << "  → Tiled 版本 (Ch05) 可以把 AI 提升到 ~TILE_SIZE/2\n";
    std::cout << "     例如 TILE_SIZE=16 → AI≈8, TILE_SIZE=32 → AI≈16\n";

    return 0;
}
```

::: tip 第 5 章预告
这个基础矩阵乘法 Kernel 虽然正确，但性能通常只有 GPU 峰值的 **1-5%**。第 5 章将引入 **Shared Memory Tiling** 技术：把 A 和 B 的子块加载到片上的 Shared Memory 中复用，将全局内存访问量降低约 `TILE_SIZE` 倍，性能可提升 **10-30 倍**。
:::

---

## 3.5 2D 索引的完整推导

为了确保你完全理解 2D 索引映射，让我们从头推导一遍。

### 问题设定

- 数据：`width × height` 的矩阵（行优先布局）
- Block 大小：`BLOCK_W × BLOCK_H`
- Grid 大小：`ceil(width/BLOCK_W) × ceil(height/BLOCK_H)`

### 推导过程

```
Step 1: 确定 Block 在 Grid 中的位置
  Block 的行号: blockIdx.y   (取值范围 0..gridDim.y-1)
  Block 的列号: blockIdx.x   (取值范围 0..gridDim.x-1)

Step 2: 确定 Thread 在 Block 中的位置
  Block 内行号: threadIdx.y  (取值范围 0..blockDim.y-1)
  Block 内列号: threadIdx.x  (取值范围 0..blockDim.x-1)

Step 3: 计算全局坐标
  全局列号 col = blockIdx.x * blockDim.x + threadIdx.x
  全局行号 row = blockIdx.y * blockDim.y + threadIdx.y

Step 4: 线性化为 1D 索引 (行优先)
  index = row * width + col

Step 5: 边界检查
  if (col < width && row < height)
```

### 具体数字示例

```
数据: 10 × 6 矩阵, Block: 4 × 4

Grid 配置:
  gridDim.x = ceil(10/4) = 3
  gridDim.y = ceil(6/4)  = 2

Grid 布局:
       col: 0  1  2  3  4  5  6  7  8  9  (10)(11)
     ┌─────────────┬─────────────┬─────────────┐
row 0│ B(0,0)      │ B(1,0)      │ B(2,0)      │
    1│             │             │             │
    2│             │             │          XX │← 越界
    3│             │             │          XX │
     ├─────────────┼─────────────┼─────────────┤
    4│ B(0,1)      │ B(1,1)      │ B(2,1)      │
    5│             │             │             │
  (6)│ XXXXXXXXXX  │ XXXXXXXXXXX │ XXXXXXXXXXX │← 越界
  (7)│ XXXXXXXXXX  │ XXXXXXXXXXX │ XXXXXXXXXXX │
     └─────────────┴─────────────┴─────────────┘

XX = 超出数据范围的线程, 被 if (col<10 && row<6) 跳过

例: Block(2,1) 中的 Thread(1,0):
  col = 2*4 + 1 = 9
  row = 1*4 + 0 = 4
  index = 4 * 10 + 9 = 49
  → 处理矩阵中第 49 号元素 (第 4 行第 9 列)

例: Block(2,0) 中的 Thread(2,3):
  col = 2*4 + 2 = 10  > width(10)
  → 越界! if 条件不满足, 跳过
```

---

## 3.6 推广到 3D

虽然 3D Grid/Block 在本书中使用较少，但在某些场景下很有用（如 3D 体素处理、视频逐帧处理）。

### 3D 配置

```c
dim3 blockDim(8, 8, 4);    // 8×8×4 = 256 threads
dim3 gridDim(
    (dimX + 7) / 8,
    (dimY + 7) / 8,
    (dimZ + 3) / 4
);
```

### 3D 索引计算

```c
int x = blockIdx.x * blockDim.x + threadIdx.x;
int y = blockIdx.y * blockDim.y + threadIdx.y;
int z = blockIdx.z * blockDim.z + threadIdx.z;

if (x < dimX && y < dimY && z < dimZ) {
    int index = z * (dimY * dimX) + y * dimX + x;
    // ...
}
```

### CUDA 维度限制

| 维度 | Grid 上限 | Block 上限 |
|------|----------|-----------|
| x | 2³¹ - 1 (\(\approx\) 21 亿) | 1024 |
| y | 65535 | 1024 |
| z | 65535 | 64 |
| **总线程数/Block** | — | **1024** |

注意：Block 的三个维度 **乘积** 不能超过 1024。所以 `(32, 32, 2)` 是合法的（32×32×2 = 2048 > 1024 **不合法**），但 `(16, 16, 4)` 是合法的（16×16×4 = 1024）。

---

## 3.7 选择 Grid/Block 维度的策略

在实际开发中，如何选择 Grid 和 Block 的维度？

### 原则 1：匹配数据维度

```
数据维度          推荐 Block 形状        示例
───────────────────────────────────────────────
1D (向量)         (256, 1, 1)           向量加法
2D (矩阵/图像)    (16, 16, 1)           矩阵乘法, 图像处理
3D (体数据)       (8, 8, 4)             体素渲染, 3D 卷积
```

### 原则 2：保持 Warp 对齐

Block 的 **x 维度** 应该是 32 的倍数（或至少是 Warp 友好的），因为 Warp 是沿 x 方向依次填充的：

```
Block (16, 16) 中 256 个线程的 Warp 分配:

threadIdx 线性化:
  linearIdx = threadIdx.z * (blockDim.y * blockDim.x)
            + threadIdx.y * blockDim.x
            + threadIdx.x

Warp 0: threadIdx (0,0)~(15,0), (0,1)~(15,1)   → 行 0-1 的 32 个线程
Warp 1: threadIdx (0,2)~(15,2), (0,3)~(15,3)   → 行 2-3 的 32 个线程
...
Warp 7: threadIdx (0,14)~(15,14), (0,15)~(15,15) → 行 14-15 的 32 个线程

→ 每个 Warp 覆盖 2 行 × 16 列
→ Warp 内的线程在 x 方向连续, 有利于内存合并访问
```

### 原则 3：Grid 大小足够覆盖数据

```c
dim3 gridDim(
    (data_width  + blockDim.x - 1) / blockDim.x,
    (data_height + blockDim.y - 1) / blockDim.y
);
```

永远用 **向上取整**，然后在 Kernel 中做边界检查。

---

## 3.8 常见的索引错误与调试

### 错误 1：x/y 方向搞反

```c
// ✗ 错误: col 用了 y, row 用了 x
int col = blockIdx.y * blockDim.y + threadIdx.y;
int row = blockIdx.x * blockDim.x + threadIdx.x;

// ✓ 正确: x → col (列), y → row (行)
int col = blockIdx.x * blockDim.x + threadIdx.x;
int row = blockIdx.y * blockDim.y + threadIdx.y;
```

**助记**：x 是最内维（变化最快 → 列），y 是次内维（行）。如果还是记不住，可以定义宏使语义更清晰：

```c
#define COL (blockIdx.x * blockDim.x + threadIdx.x)
#define ROW (blockIdx.y * blockDim.y + threadIdx.y)
```

### 错误 2：线性化公式中 width 和 height 搞反

```c
// ✗ 错误: 应该乘 width (列数), 而不是 height (行数)
int idx = row * height + col;

// ✓ 正确
int idx = row * width + col;
```

**助记**：行优先布局中，跳过一行需要跨过 `width` 个元素。

### 错误 3：Grid 配置中 x/y 对应的维度搞反

```c
// ✗ 错误
dim3 gridDim((height + 15) / 16, (width + 15) / 16);

// ✓ 正确: gridDim.x 对应 width, gridDim.y 对应 height
dim3 gridDim((width + 15) / 16, (height + 15) / 16);
```

### 调试技巧

当矩阵结果不对时，先用 **小矩阵**（如 4×4）检查：

```c
// 在 Kernel 中打印 (仅调试用, 生产代码删除)
if (row == 0 && col == 0) {
    printf("Block(%d,%d) Thread(%d,%d) → row=%d col=%d idx=%d\n",
           blockIdx.x, blockIdx.y, threadIdx.x, threadIdx.y,
           row, col, row * width + col);
}
```

---

## 3.9 本章总结

### 核心概念速查

| 概念 | 要点 |
|------|------|
| **`dim3`** | CUDA 三维向量类型，用于配置 Grid/Block 维度 |
| **2D 线程 ID** | `col = blockIdx.x * blockDim.x + threadIdx.x`<br>`row = blockIdx.y * blockDim.y + threadIdx.y` |
| **行优先布局** | `index = row * width + col`，同一行元素内存连续 |
| **边界检查** | `if (col < width && row < height)` |
| **Block 大小** | 2D 常用 16×16 或 32×32，保持 x 维度为 32 倍数 |
| **数据并行映射** | N 重循环 → N 维 Grid，最内层循环可保留在线程内 |

### 两个案例对比

| 对比项 | 彩色转灰度 | 矩阵乘法 |
|--------|-----------|---------|
| 每线程计算量 | 3 次乘法 + 2 次加法 | K 次乘法 + K 次加法 |
| 每线程访存量 | 3 字节读 + 1 字节写 | 2K 次 float 读 + 1 次写 |
| 计算强度 | 很低（内存受限） | 低（基础版本内存受限） |
| 线程间数据复用 | 无 | 有（A 的行被 N 个线程共享）|
| 优化方向 | 带宽优化、合并访问 | **Shared Memory Tiling**（Ch5） |

### 从 1D 到 2D 的模式总结

```
1D Kernel 模板 (向量):
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) { /* ... */ }

2D Kernel 模板 (矩阵/图像):
  int col = blockIdx.x * blockDim.x + threadIdx.x;
  int row = blockIdx.y * blockDim.y + threadIdx.y;
  if (col < width && row < height) {
      int idx = row * width + col;
      /* ... */
  }

3D Kernel 模板 (体数据):
  int x = blockIdx.x * blockDim.x + threadIdx.x;
  int y = blockIdx.y * blockDim.y + threadIdx.y;
  int z = blockIdx.z * blockDim.z + threadIdx.z;
  if (x < dimX && y < dimY && z < dimZ) {
      int idx = z * (dimY * dimX) + y * dimX + x;
      /* ... */
  }
```

---

::: tip 下一章预告
[Ch04: Compute Architecture and Scheduling](./ch04) 将深入 GPU 的硬件架构——SM 结构、Warp 执行模型、Warp 调度与延迟隐藏机制、Occupancy 的定义与影响。这是理解"为什么你的 Kernel 性能不好"的关键一章。
:::

---

## 扩展思考

::: details 思考题 1：对于 1000×1000 的矩阵乘法，GPU 比 CPU 快多少？
粗略估算：

**CPU**（单核 i9, ~50 GFLOPS FP32）：
- 计算量 = 2 × 1000³ = 2 GFLOP
- 耗时 ≈ 2 / 50 = 40 ms

**GPU**（A100, 基础 Kernel ~500 GFLOPS，约峰值 2.5%）：
- 耗时 ≈ 2 / 500 = 4 ms
- 但加上 PCIe 传输 ~3 MB × 2 / 32 GB/s ≈ 0.2 ms，几乎可忽略

基础 Kernel 约快 **10 倍**。但如果用 Tiled 版本（~5 TFLOPS）可以快 **100 倍**，cuBLAS（~15 TFLOPS+）可以快 **300 倍**。

这说明：GPU 的收益 **高度依赖 Kernel 优化程度**。写一个正确但朴素的 Kernel 只是起点。
:::

::: details 思考题 2：为什么 CUDA 中 x 是最内维度而不是最外维度？
这与 GPU 硬件的 Warp 组织方式一致。在一个 Block 中，线程被线性化后按 **32 个一组** 划分为 Warp：

```
线性化顺序: threadIdx.x 变化最快

Block(16,16) 的线程线性化:
  linearIdx = threadIdx.y * 16 + threadIdx.x

  threadIdx.x: 0  1  2  ... 15  0  1  2  ... 15  ...
  threadIdx.y: 0  0  0  ...  0  1  1  1  ...  1  ...
  linearIdx:   0  1  2  ... 15 16 17 18  ... 31  ...
               └───── Warp 0 ──────┘└──── Warp 1 ─┘
```

Warp 0 包含了 `threadIdx.y=0, x=0..15` 和 `threadIdx.y=1, x=0..15` 的线程。这些线程的 `col` 值是连续的——访问矩阵同一行中连续的列，实现了 **内存合并访问**。

如果 x 是外层维度，同一个 Warp 中的线程会访问不连续的内存地址，性能会差很多。
:::

::: details 思考题 3：彩色转灰度中，交错布局和平面布局哪个对 GPU 更友好？
**平面布局 (SoA) 对 GPU 更友好**。

交错布局：`[R₀ G₀ B₀ R₁ G₁ B₁ ...]`
- 一个 Warp 中 32 个线程要读取 `R₀ R₁ ... R₃₁`，它们在内存中间隔为 3——**不连续**
- 需要 3 次内存事务才能读完 32 个 R 值

平面布局：`[R₀ R₁ R₂ ... | G₀ G₁ G₂ ... | B₀ B₁ B₂ ...]`
- 一个 Warp 读取 `R₀ R₁ ... R₃₁`，在内存中 **完全连续**
- 只需 1 次内存事务

这就是为什么深度学习框架（PyTorch, TensorFlow）中图像张量默认是 NCHW（通道优先 = 平面布局）而非 NHWC（通道最后 = 交错布局）。

不过，对于彩色转灰度这种每个像素只访问一次的操作，差距不大。对于卷积等需要邻域操作的 Kernel，布局选择的影响会更显著。
:::

::: details 思考题 4：基础矩阵乘法中，计算 C 的同一行的不同线程是否可以共享 A 的行数据？如何共享？
它们当然 **应该** 共享，但在基础版本中 **没有共享**——每个线程独立地从全局内存读取 A 的整行。

共享的方法就是 **Shared Memory Tiling**（第 5 章的核心内容）：

1. 一个 Block 中的所有线程 **协作** 将 A 和 B 的一个子块（Tile）从全局内存加载到 Shared Memory
2. 所有线程从 Shared Memory（而非全局内存）读取数据做计算
3. Shared Memory 的带宽比全局内存高 ~10 倍，延迟低 ~100 倍

```
基础版本: 每线程独立读全局内存

Thread(row, 0): 读 A[row][0..K-1] 从全局内存
Thread(row, 1): 读 A[row][0..K-1] 从全局内存  ← 完全重复!
Thread(row, 2): 读 A[row][0..K-1] 从全局内存  ← 完全重复!

Tiled 版本: Block 内共享

Block 内所有线程:
  → 协作加载 A 的一个 Tile 到 Shared Memory (1次全局内存读)
  → 所有线程从 Shared Memory 读取 (极快)
  → A 的一行数据只被读取 1 次, 而非 blockDim.x 次
```

Shared Memory 是 CUDA 性能优化的核心工具之一。
:::
