---
title: "PMPP Ch04: Compute Architecture and Scheduling"
date: 2026-02-19
---

# Ch04: Compute Architecture and Scheduling

<p style="color: var(--vp-c-text-2); font-size: 14px;">
2026-02-19 &nbsp;·&nbsp; PMPP 专栏 &nbsp;·&nbsp; 第四章
</p>

> **本章信息**
> - **书名**: Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)
> - **作者**: David B. Kirk, Wen-mei W. Hwu
> - **范围**: 第 4 章 Compute Architecture and Scheduling

## 一句话总结

本章深入 GPU 的 **硬件微架构**——Streaming Multiprocessor（SM）内部结构、Warp 的概念与执行模型、Warp 调度如何隐藏延迟、资源分配如何决定 Occupancy。这一章回答了一个关键问题：**GPU 有几千个核心，性能却不一定好——瓶颈到底在哪里？**

---

## 4.1 GPU 硬件架构总览

### 从芯片到线程

一块 GPU 芯片的结构可以自顶向下分为四层：

```
GPU 硬件架构层次:

┌──────────────────────────────────────────────────────────────┐
│                        GPU 芯片                              │
│                                                              │
│  ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐    ┌──────┐ │
│  │ SM 0 │ │ SM 1 │ │ SM 2 │ │ SM 3 │ │ SM 4 │ .. │SM N-1│ │
│  └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘    └──┬───┘ │
│     │        │        │        │        │            │     │
│  ┌──┴────────┴────────┴────────┴────────┴────────────┴──┐  │
│  │                    L2 Cache                           │  │
│  └──────────────────────┬───────────────────────────────┘  │
│                         │                                   │
│  ┌──────────────────────┴───────────────────────────────┐  │
│  │              Global Memory (HBM)                      │  │
│  │              容量: 数十 GB    带宽: ~TB/s              │  │
│  └───────────────────────────────────────────────────────┘  │
└──────────────────────────────────────────────────────────────┘

典型 SM 数量:
  V100: 80 SMs    A100: 108 SMs    H100: 132 SMs
```

### SM 内部结构

每个 SM（Streaming Multiprocessor）是一个独立的处理器，包含完整的计算资源：

```
SM 内部结构 (以 A100 为例):

┌─────────────────────────────────────────────────────────┐
│                    Streaming Multiprocessor              │
│                                                         │
│  ┌──────────────────────────────────────────────────┐   │
│  │              Warp Scheduler (×4)                  │   │
│  │  每周期选择 4 个就绪 Warp, 各发射 1 条指令        │   │
│  └──────────────────────────────────────────────────┘   │
│                                                         │
│  ┌────────────────┐  ┌────────────────┐                 │
│  │  INT32 单元     │  │  FP32 单元      │                │
│  │  (64 个)        │  │  (64 个)        │                │
│  └────────────────┘  └────────────────┘                 │
│                                                         │
│  ┌────────────────┐  ┌────────────────┐                 │
│  │  FP64 单元     │  │  Tensor Core    │                │
│  │  (32 个)       │  │  (4 个, 第三代)  │                │
│  └────────────────┘  └────────────────┘                 │
│                                                         │
│  ┌────────────────┐  ┌────────────────┐                 │
│  │  LD/ST 单元    │  │  SFU           │                 │
│  │  (32 个)       │  │  (16 个)       │                 │
│  │  加载/存储     │  │  特殊函数      │                 │
│  └────────────────┘  │  sin/cos/rsqrt │                 │
│                      └────────────────┘                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Register File (寄存器堆)                         │   │
│  │  65536 个 32-bit 寄存器 (256 KB)                  │   │
│  └──────────────────────────────────────────────────┘   │
│                                                         │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Shared Memory / L1 Cache (可配置)                │   │
│  │  A100: 最大 164 KB Shared Memory                  │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

**关键硬件参数对比**：

| 参数 | V100 (Volta) | A100 (Ampere) | H100 (Hopper) |
|------|-------------|--------------|---------------|
| SM 数量 | 80 | 108 | 132 |
| FP32 Cores / SM | 64 | 64 | 128 |
| FP32 总 Cores | 5120 | 6912 | 16896 |
| Tensor Cores / SM | 8 (Gen1) | 4 (Gen3) | 4 (Gen4) |
| Register / SM | 256 KB | 256 KB | 256 KB |
| Shared Mem / SM | 最大 96 KB | 最大 164 KB | 最大 228 KB |
| Max Threads / SM | 2048 | 2048 | 2048 |
| Max Warps / SM | 64 | 64 | 64 |
| Warp Schedulers / SM | 4 | 4 | 4 |

---

## 4.2 Block 到 SM 的映射

### Block 分配机制

当 Host 启动一个 Kernel 时，GPU 的 **Giga Thread Engine（全局调度器）** 负责将 Grid 中的 Block 分配到各个 SM 上：

```
Block → SM 分配:

Grid (由 Kernel 启动产生):
┌────┬────┬────┬────┬────┬────┬────┬────┬────┐
│ B0 │ B1 │ B2 │ B3 │ B4 │ B5 │ B6 │ B7 │ B8 │ ...
└────┴────┴────┴────┴────┴────┴────┴────┴────┘

            │ Giga Thread Engine 分配
            ▼

SM 0         SM 1         SM 2         SM 3
┌──────┐    ┌──────┐    ┌──────┐    ┌──────┐
│ B0   │    │ B1   │    │ B2   │    │ B3   │
│ B4   │    │ B5   │    │ B6   │    │ B7   │
│ B8   │    │      │    │      │    │      │
└──────┘    └──────┘    └──────┘    └──────┘

规则:
  1. 一个 Block 只在一个 SM 上执行 (不跨 SM)
  2. 一个 SM 可以同时驻留多个 Block
  3. Block 执行完毕后, 其资源立刻释放, 新 Block 被调度上来
  4. Block 间的执行顺序没有保证
```

### 为什么 Block 间不能同步？

一个至关重要的设计决策：**CUDA 不保证 Block 的执行顺序，也不提供 Block 间的同步原语**（除了 Cooperative Groups 中的全局同步）。

原因：如果 Block 间可以互相等待，而一个 SM 上的 Block 在等另一个还没被调度到 SM 的 Block——就 **死锁** 了。不保证执行顺序是确保 **任意 Block 数量的 Kernel 都能正确执行** 的前提。

```
死锁场景 (如果允许 Block 间同步):

假设只有 2 个 SM, 但有 4 个 Block:

SM 0: [B0, B2]    SM 1: [B1, B3]

如果 B0 等待 B3 的结果, 而 B3 等待 B0 的结果:
  B0 在 SM 0 上等 B3 → B3 在 SM 1 上等 B0 → 死锁!

更糟的情况: 如果 SM 只有 2 个, 但 B0 等待 B4:
  B4 还没被调度到任何 SM 上 (SM 都满了)
  → B4 永远不会执行 → B0 永远等待 → 永久死锁!
```

::: warning Block 执行顺序是不确定的
不要假设 Block 0 一定在 Block 1 之前执行。不要假设同一个 SM 上的 Block 有特定的执行先后。任何依赖 Block 执行顺序的代码都是 **未定义行为**。

Block 间通信只能通过：
1. Kernel 结束后的全局内存（跨 Kernel 通信）
2. 原子操作（不保证顺序，但保证原子性）
3. Cooperative Groups 的 `grid.sync()`（需要特殊启动方式）
:::

---

## 4.3 Warp：GPU 的基本执行单位

### 什么是 Warp？

**Warp** 是 GPU 执行的最小单位——**32 个线程组成一个 Warp，这 32 个线程在同一时刻执行完全相同的指令**。

```
Block 到 Warp 的分解:

Block (blockDim = 256):
┌──────────────────────────────────────────────────┐
│ Thread 0  1  2  3  ... 31 │ 32 33 34 ... 63 │ ...│
│ └──── Warp 0 ────────────┘ └── Warp 1 ──────┘    │
│                                                    │
│ Thread 64 ... 95 │ 96 ... 127 │ ... │ 224 ... 255 │
│ └── Warp 2 ─────┘ └─ Warp 3 ─┘     └── Warp 7 ──┘│
└──────────────────────────────────────────────────┘

256 个线程 → 256 / 32 = 8 个 Warp
```

Warp 的划分是 **确定性的**，按线程的线性索引（`linearIdx = threadIdx.z * blockDim.y * blockDim.x + threadIdx.y * blockDim.x + threadIdx.x`）分组：

- Warp 0：线程 0-31
- Warp 1：线程 32-63
- ...
- Warp k：线程 32k 到 32k+31

### SIMT 执行模型

GPU 采用的是 **SIMT（Single Instruction, Multiple Threads）** 执行模型——一个 Warp 中的 32 个线程在同一个时钟周期执行同一条指令，但作用于各自不同的数据。

```
SIMT 执行 (以向量加法为例):

Warp 0 的 32 个线程同时执行:

时钟周期 1: 所有 32 个线程执行 "计算 i = blockDim.x * blockIdx.x + threadIdx.x"
            Thread 0: i = 0    Thread 1: i = 1    ... Thread 31: i = 31

时钟周期 2: 所有 32 个线程执行 "从全局内存加载 A[i]"
            Thread 0: 加载 A[0]  Thread 1: 加载 A[1]  ... Thread 31: 加载 A[31]
            (如果地址连续 → 合并为 1 次内存事务!)

时钟周期 3: 所有 32 个线程执行 "从全局内存加载 B[i]"

时钟周期 4: 所有 32 个线程执行 "计算 C[i] = A[i] + B[i]"

时钟周期 5: 所有 32 个线程执行 "存储 C[i] 到全局内存"
```

::: tip SIMT vs SIMD
SIMT 和 SIMD（Single Instruction, Multiple Data）概念类似但有区别：

- **SIMD**（如 CPU 的 AVX-512）：显式向量指令，程序员或编译器必须把数据打包成向量
- **SIMT**：程序员编写 **标量代码**（每个线程处理一个元素），硬件自动将 32 个线程组合成一个 Warp 并行执行

SIMT 对程序员更友好——你写的是单线程逻辑，硬件负责并行化。但你需要了解 Warp 的存在，才能写出高效代码（如避免 Warp Divergence）。
:::

---

## 4.4 Warp Divergence：条件分支的代价

### 什么是 Warp Divergence？

当一个 Warp 中的线程遇到 `if-else` 等条件分支时，如果不同线程走了不同的分支路径，就产生了 **Warp Divergence（Warp 分化）**。

由于 Warp 中的 32 个线程必须执行同一条指令，硬件处理 Divergence 的方式是：**先执行 if 分支（禁用走 else 的线程），再执行 else 分支（禁用走 if 的线程）**——两个分支被串行化执行。

```
Warp Divergence 示例:

代码:
  if (threadIdx.x < 16)
      A();   // 分支 1
  else
      B();   // 分支 2

没有 Divergence 的情况 (Warp 内所有线程走同一分支):
  Warp 0 (线程 0-31):  所有线程 threadIdx.x < 16?
    → 线程 0-15: 是    线程 16-31: 否
    → 发生 Divergence!

硬件执行过程:
  Pass 1: 执行 A()
    线程 0-15: 活跃 ✓ (执行 A)
    线程 16-31: 禁用 ✗ (空等)

  Pass 2: 执行 B()
    线程 0-15: 禁用 ✗ (空等)
    线程 16-31: 活跃 ✓ (执行 B)

  汇合点: 所有线程重新统一

    时间 ──────────────────────────────►
         │ Pass 1: A()  │ Pass 2: B()  │
    t0-15│ ████████████  │ ░░░░░░░░░░░  │ ← 50% 利用率
   t16-31│ ░░░░░░░░░░░  │ ████████████  │ ← 50% 利用率

    → 总执行时间 = time(A) + time(B), 而非 max(A, B)
    → 最坏情况下性能降低一半!
```

### Divergence 只发生在 Warp 内部

**不同 Warp 走不同分支不算 Divergence**——因为 Warp 之间本来就是独立调度的。

```
不造成 Divergence 的分支模式:

if (blockIdx.x == 0)   // 不同 Block → 不同 Warp → 无 Divergence
    processBlockZero();
else
    processOtherBlocks();

if (threadIdx.x / 32 == 0)  // 按 Warp 划分 → 同一 Warp 走同一分支 → 无 Divergence
    warpZeroCode();
else
    otherWarpsCode();
```

### 量化 Divergence 的影响

```cpp-run title="Warp Divergence 性能影响估算"
#include <iostream>
#include <iomanip>

int main() {
    std::cout << "=== Warp Divergence 对性能的影响 ===\n\n";

    std::cout << "假设: if 分支和 else 分支各需要 T 个时钟周期\n\n";

    std::cout << std::setw(30) << "场景"
              << std::setw(14) << "执行时间"
              << std::setw(14) << "利用率"
              << std::setw(14) << "性能损失" << "\n";
    std::cout << std::string(72, '-') << "\n";

    struct Case {
        const char* name;
        int if_threads;
        int else_threads;
    };

    Case cases[] = {
        {"无 Divergence (全走 if)",       32, 0},
        {"无 Divergence (全走 else)",      0, 32},
        {"50/50 分裂",                    16, 16},
        {"1 个线程走 else",               31, 1},
        {"仅 1 个线程走 if",               1, 31},
    };

    for (auto& c : cases) {
        int paths = (c.if_threads > 0 ? 1 : 0) + (c.else_threads > 0 ? 1 : 0);
        double utilization;
        double time_units;

        if (paths <= 1) {
            time_units = 1.0;
            utilization = 1.0;
        } else {
            time_units = 2.0;
            utilization = 0.5;
        }

        double perf_loss = (1.0 - 1.0 / time_units) * 100.0;

        std::cout << std::setw(30) << c.name
                  << std::setw(11) << std::fixed << std::setprecision(0)
                  << time_units << " T"
                  << std::setw(11) << std::setprecision(0)
                  << utilization * 100 << " %"
                  << std::setw(11) << std::setprecision(0)
                  << perf_loss << " %" << "\n";
    }

    std::cout << "\n关键洞察:\n";
    std::cout << "  - 即使只有 1 个线程走了不同分支, 整个 Warp 也要付出双倍时间\n";
    std::cout << "  - Divergence 的代价与 '分裂比例' 无关, 只取决于分支数\n";
    std::cout << "  - 多层嵌套 if-else 可以导致更多路径, 性能更差\n";

    return 0;
}
```

### 减少 Divergence 的策略

**策略 1：让分支以 Warp 为边界对齐**

```c
// ✗ 坏: 同一 Warp 中有些线程走 if, 有些走 else
if (threadIdx.x % 2 == 0) { A(); } else { B(); }
// Warp 0 中线程 0,2,4..走 A, 线程 1,3,5..走 B → 100% Divergence

// ✓ 好: 每 32 个线程一组, 走同一分支
if ((threadIdx.x / 32) % 2 == 0) { A(); } else { B(); }
// Warp 0 全走 A, Warp 1 全走 B → 0% Divergence
```

**策略 2：用算术替代分支**

```c
// ✗ 有 Divergence
if (x > 0)
    y = x;
else
    y = 0;

// ✓ 无分支
y = (x > 0) ? x : 0;   // 编译器可能优化为 predicated 指令
y = fmaxf(x, 0.0f);     // 用内置函数, 保证无分支
```

**策略 3：重新组织数据（如排序），让相邻线程走相同路径**

---

## 4.5 Warp 调度与延迟隐藏

### GPU 的秘密武器：零成本上下文切换

GPU 性能的核心秘诀不是"快"，而是 **永远不停**。当一个 Warp 因为等待内存数据而 stall 时，SM 的 Warp Scheduler 会立即切换到另一个就绪的 Warp 继续执行——这个切换 **零延迟**（因为每个 Warp 的寄存器状态始终驻留在 SM 上，不需要保存/恢复上下文）。

```
Warp 调度与延迟隐藏:

   时间 ────────────────────────────────────────────────────────►

Warp 0: [计算][  等内存...............  ][计算][  等内存...  ][计算]
Warp 1:       [计算][  等内存.............  ][计算][等内存][计算]
Warp 2:             [计算][  等内存...........  ][计算][计算]
Warp 3:                   [计算][  等内存.......  ][计算]
   ...

SM 执行: [W0 ][W1 ][W2 ][W3 ][W0 ][W1 ][W2 ][W3 ][W0 ][W1 ]...
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          SM 始终在执行某个 Warp, 延迟被完全隐藏!

关键条件: 必须有足够多的就绪 Warp 来填满等待时间
```

### 延迟隐藏的数学

假设：
- 全局内存访问延迟：\(L\) 个时钟周期（A100 上约 200-800 cycles）
- 每个 Warp 每次执行 \(C\) 个时钟周期后就需要等内存
- SM 上有 \(W\) 个活跃 Warp

要完全隐藏延迟，需要：

$$
W \times C \geq L
$$

即：当一个 Warp 发出内存请求后，其他 Warp 执行的时间之和必须 \(\geq\) 内存延迟。

```
延迟隐藏条件的直觉:

内存延迟 L = 200 cycles
每个 Warp 执行 C = 10 cycles 后 stall

需要 Warp 数 W ≥ L / C = 200 / 10 = 20 个 Warp

如果只有 4 个 Warp:
  4 × 10 = 40 cycles < 200 cycles
  → 有 200 - 40 = 160 cycles 的空闲时间 → SM 被 stall!

如果有 20 个 Warp:
  20 × 10 = 200 cycles = 200 cycles
  → 延迟刚好被完全隐藏 → SM 全速运转!

如果有 40 个 Warp:
  40 × 10 = 400 cycles > 200 cycles
  → 不仅延迟被隐藏, 还有额外的调度余量 → 最佳状态
```

::: tip 延迟隐藏 vs CPU 缓存
CPU 通过大缓存（数十 MB L3）来 **减少** 延迟。GPU 通过大量 Warp 来 **隐藏** 延迟。两种策略对晶体管的使用完全不同——CPU 把晶体管给了缓存，GPU 把晶体管给了计算单元和寄存器。
:::

---

## 4.6 资源分配与 Occupancy

### 什么是 Occupancy？

**Occupancy（占用率）** 是 SM 上实际活跃 Warp 数与该 SM 支持的最大 Warp 数之比：

$$
\text{Occupancy} = \frac{\text{活跃 Warp 数}}{\text{SM 最大 Warp 数}}
$$

例如 A100 每 SM 最多 64 个 Warp（2048 线程），如果实际驻留了 32 个 Warp，则 Occupancy = 50%。

### 影响 Occupancy 的三大资源

SM 上的资源是有限的。一个 Block 驻留在 SM 上时，需要占用三种关键资源，任何一种资源耗尽都会限制能驻留的 Block 数量：

```
SM 资源分配 (A100):

┌──────────────────────────────────────────────────┐
│                     SM                            │
│                                                   │
│  ┌──────────────┐ 限制 1: 寄存器                  │
│  │ Register File│ 总量: 65536 个 (256 KB)         │
│  │              │ 每线程用 N 个 → 每 Block 用      │
│  │              │ N × blockDim 个                  │
│  └──────────────┘                                 │
│                                                   │
│  ┌──────────────┐ 限制 2: Shared Memory           │
│  │ Shared Mem   │ 总量: 最大 164 KB               │
│  │              │ 每 Block 用 S KB                 │
│  └──────────────┘                                 │
│                                                   │
│  ┌──────────────┐ 限制 3: Thread Slots            │
│  │ Thread Slots │ 总量: 2048 个线程 (64 Warps)    │
│  │              │ 每 Block 占 blockDim 个          │
│  └──────────────┘                                 │
│                                                   │
│  ┌──────────────┐ 限制 4: Block Slots             │
│  │ Block Slots  │ 总量: 32 个 Block               │
│  └──────────────┘                                 │
│                                                   │
│  最终 Occupancy = min(寄存器限制, SM限制, 线程限制, │
│                       Block限制) / 64 Warps       │
└──────────────────────────────────────────────────┘
```

### 资源限制的具体计算

让我们通过一个详细的例子，计算 Occupancy：

**场景**：A100 GPU，Kernel 配置如下：
- `blockDim = 256`（8 个 Warp）
- 每线程使用 32 个寄存器
- 每 Block 使用 4 KB Shared Memory

**A100 SM 资源上限**：
- 最大 2048 线程 / 64 Warp / 32 Block
- 65536 个寄存器
- 164 KB Shared Memory

```
Step 1: Thread Slot 限制
  每 Block 256 线程, SM 最大 2048 线程
  → 最多 2048 / 256 = 8 个 Block
  → 8 × 256 = 2048 线程 = 64 Warp → Occupancy = 100%

Step 2: Block Slot 限制
  SM 最大 32 个 Block
  → 不是瓶颈 (8 < 32)

Step 3: 寄存器限制
  每线程 32 个寄存器, 每 Block 256 线程
  → 每 Block 需要 32 × 256 = 8192 个寄存器
  → SM 有 65536 个寄存器
  → 最多 65536 / 8192 = 8 个 Block → 不是瓶颈

Step 4: Shared Memory 限制
  每 Block 4 KB, SM 有 164 KB
  → 最多 164 / 4 = 41 个 Block → 不是瓶颈

最终: 瓶颈是 Thread Slot, Occupancy = 100%
```

现在把每线程寄存器数增加到 64：

```
Step 3 (修改): 寄存器限制
  每线程 64 个寄存器, 每 Block 256 线程
  → 每 Block 需要 64 × 256 = 16384 个寄存器
  → SM 有 65536 个寄存器
  → 最多 65536 / 16384 = 4 个 Block
  → 4 × 256 = 1024 线程 = 32 Warp → Occupancy = 50%

瓶颈从 Thread Slot 变成了寄存器!
```

```cpp-run title="Occupancy 计算器"
#include <iostream>
#include <iomanip>
#include <algorithm>

int main() {
    std::cout << "=== CUDA Occupancy 计算器 ===\n\n";

    // SM 资源上限 (A100)
    const int MAX_THREADS_PER_SM = 2048;
    const int MAX_BLOCKS_PER_SM  = 32;
    const int MAX_REGS_PER_SM    = 65536;
    const int MAX_SMEM_PER_SM_KB = 164;
    const int WARP_SIZE = 32;
    const int MAX_WARPS = MAX_THREADS_PER_SM / WARP_SIZE;

    std::cout << "SM 资源 (A100): "
              << MAX_THREADS_PER_SM << " threads, "
              << MAX_REGS_PER_SM << " regs, "
              << MAX_SMEM_PER_SM_KB << " KB smem, "
              << MAX_BLOCKS_PER_SM << " blocks\n\n";

    struct Config {
        const char* name;
        int block_size;
        int regs_per_thread;
        int smem_per_block_kb;
    };

    Config configs[] = {
        {"轻量 Kernel (如 vecadd)",       256, 16, 0},
        {"中等 Kernel (如基础 matmul)",    256, 32, 4},
        {"寄存器密集 Kernel",             256, 64, 4},
        {"Shared Memory 密集 Kernel",     256, 32, 48},
        {"大 Block + 高资源",             512, 48, 32},
        {"小 Block",                      64,  32, 0},
        {"最大 Block",                   1024, 24, 0},
    };

    std::cout << std::setw(32) << "配置"
              << std::setw(8) << "Block"
              << std::setw(8) << "Regs"
              << std::setw(8) << "Smem"
              << std::setw(10) << "Blocks"
              << std::setw(10) << "Warps"
              << std::setw(10) << "Occup."
              << std::setw(12) << "瓶颈" << "\n";
    std::cout << std::string(98, '-') << "\n";

    for (auto& c : configs) {
        int warps_per_block = (c.block_size + WARP_SIZE - 1) / WARP_SIZE;

        int blocks_by_threads = MAX_THREADS_PER_SM / c.block_size;
        int blocks_by_slots   = MAX_BLOCKS_PER_SM;
        int blocks_by_regs    = MAX_REGS_PER_SM / (c.regs_per_thread * c.block_size);
        int blocks_by_smem    = (c.smem_per_block_kb > 0)
                                ? MAX_SMEM_PER_SM_KB / c.smem_per_block_kb
                                : 9999;

        int active_blocks = std::min({blocks_by_threads, blocks_by_slots,
                                      blocks_by_regs, blocks_by_smem});
        int active_warps = active_blocks * warps_per_block;
        double occupancy = (double)active_warps / MAX_WARPS * 100.0;

        const char* bottleneck = "threads";
        int min_val = blocks_by_threads;
        if (blocks_by_slots < min_val)  { min_val = blocks_by_slots;  bottleneck = "block slots"; }
        if (blocks_by_regs < min_val)   { min_val = blocks_by_regs;   bottleneck = "registers"; }
        if (blocks_by_smem < min_val)   { min_val = blocks_by_smem;   bottleneck = "smem"; }

        std::cout << std::setw(32) << c.name
                  << std::setw(8) << c.block_size
                  << std::setw(8) << c.regs_per_thread
                  << std::setw(6) << c.smem_per_block_kb << "KB"
                  << std::setw(10) << active_blocks
                  << std::setw(10) << active_warps
                  << std::setw(8) << std::fixed << std::setprecision(0) << occupancy << "%"
                  << std::setw(12) << bottleneck << "\n";
    }

    std::cout << "\n关键洞察:\n";
    std::cout << "  - 高 Occupancy 不一定 = 高性能 (但低 Occupancy 几乎一定 = 差性能)\n";
    std::cout << "  - 寄存器和 Shared Memory 是最常见的 Occupancy 限制因素\n";
    std::cout << "  - 可以通过 __launch_bounds__ 或编译选项 -maxrregcount 控制寄存器使用\n";

    return 0;
}
```

### Occupancy 与性能的关系

**高 Occupancy 不一定意味着高性能**，但 **低 Occupancy 几乎一定意味着差性能**。

原因：Occupancy 影响的是 **延迟隐藏能力**。如果 Kernel 是计算密集型（每次内存访问后做很多计算），那么少量 Warp 也能隐藏延迟——Occupancy 不重要。如果 Kernel 是内存密集型（频繁访问全局内存），那么需要大量 Warp 来隐藏内存延迟——Occupancy 至关重要。

```
Occupancy vs 性能 (定性):

性能
│
│              ╭──────────── 计算密集型 Kernel
│             ╱              (50% Occupancy 即可接近峰值)
│           ╱
│         ╱    ╭──────────── 内存密集型 Kernel
│       ╱     ╱              (需要高 Occupancy)
│     ╱     ╱
│   ╱     ╱
│ ╱     ╱
│╱    ╱
┼───┬───┬───┬───┬───┬──── Occupancy
   25%  50%  75%  100%

规律:
  - 从 0% 到 ~50%: 性能提升显著 (Warp 太少, 延迟无法隐藏)
  - 从 50% 到 100%: 性能提升递减 (边际收益下降)
  - 某些 Kernel 在 50% Occupancy 时即可达到 90%+ 的峰值性能
```

---

## 4.7 控制 Occupancy 的手段

### 方法 1：调整 Block 大小

Block 大小直接影响 Thread Slot 的使用：

```
blockDim = 128: 每 Block 4 Warp
  SM 最多 2048/128 = 16 Block, 但 Block Slot 限制 32
  → 16 Block × 128 = 2048 线程 → 100% Occupancy

blockDim = 64: 每 Block 2 Warp
  SM 最多 2048/64 = 32 Block, Block Slot 限制 32
  → 32 Block × 64 = 2048 → 100% Occupancy

blockDim = 1024: 每 Block 32 Warp
  SM 最多 2048/1024 = 2 Block
  → 2 Block × 1024 = 2048 → 100% Occupancy
  → 但只有 2 个 Block, 调度灵活性差
```

### 方法 2：限制寄存器使用

```c
// 方法 A: 编译器指令
__global__ void __launch_bounds__(256, 4) myKernel(...) {
    // 第一个参数: 每 Block 最大线程数 = 256
    // 第二个参数: 每 SM 最少 Block 数 = 4
    // → 编译器保证寄存器使用 ≤ 65536 / (256 × 4) = 64
}
```

```bash
# 方法 B: 编译器选项
nvcc -maxrregcount=32 -o program program.cu
# 限制每线程最多使用 32 个寄存器
# 多余的会 spill 到 local memory (实际是全局内存, 很慢)
```

### 方法 3：减少 Shared Memory 使用

如果 Shared Memory 是瓶颈，可以：
- 减少 Tile 大小
- 用寄存器替代部分 Shared Memory 的角色
- 配置 Shared Memory / L1 Cache 的比例

```c
// 动态调整 Shared Memory / L1 比例
cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferShared);  // Shared 优先
cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferL1);      // L1 优先
cudaFuncSetCacheConfig(myKernel, cudaFuncCachePreferEqual);   // 均分
```

---

## 4.8 Warp 调度的细节

### 调度粒度

每个 SM 有多个 **Warp Scheduler**（A100 有 4 个）。每个调度器在每个时钟周期从就绪 Warp 池中选择一个 Warp，发射一条指令。

```
SM 中的 Warp 调度 (4 个调度器):

时钟周期 T:
  Scheduler 0: 选择 Warp 3, 发射指令 "FP32 ADD"
  Scheduler 1: 选择 Warp 7, 发射指令 "LOAD"
  Scheduler 2: 选择 Warp 12, 发射指令 "FP32 MUL"
  Scheduler 3: 选择 Warp 0, 发射指令 "STORE"

→ 4 个 Warp 在同一周期各执行一条指令
→ 4 × 32 = 128 个线程同时在推进
```

### Warp 的就绪条件

一个 Warp 必须满足以下条件才能被调度器选中：

1. **操作数就绪**：指令需要的所有输入数据已经准备好（如内存加载完成、前序指令结果可用）
2. **执行单元空闲**：该指令需要的功能单元（FP32、INT32、LD/ST、SFU 等）没有被占用

如果一个 Warp 的下一条指令的操作数还没就绪（比如在等全局内存返回数据），这个 Warp 就是 **stalled（停滞）** 的，不会被选中。

### 延迟类型

| 延迟类型 | 来源 | 典型周期数 | 隐藏难度 |
|---------|------|-----------|---------|
| 算术延迟 | FP32/INT32 运算结果 → 下一条使用它的指令 | 4-6 cycles | 容易（少量 Warp 即可） |
| 全局内存延迟 | 全局内存加载/存储 | 200-800 cycles | 困难（需要大量 Warp） |
| Shared Memory 延迟 | Shared Memory 访问 | ~20-30 cycles | 中等 |
| `__syncthreads()` | Block 内栅栏同步 | 取决于最慢线程 | 中等 |

全局内存延迟是最大的——200-800 个时钟周期意味着 SM 必须有大量就绪 Warp 才能保持忙碌。这也是为什么 Occupancy 对内存密集型 Kernel 至关重要。

---

## 4.9 查询和使用 Occupancy API

CUDA 提供了 Runtime API 来程序化地查询 Occupancy：

```c
// 计算给定 Kernel 和 Block 大小的理论 Occupancy
int maxActiveBlocks;
cudaOccupancyMaxActiveBlocksPerMultiprocessor(
    &maxActiveBlocks,
    myKernel,          // Kernel 函数指针
    blockSize,         // Block 大小
    dynamicSmemSize    // 动态 Shared Memory 大小
);
float occupancy = (float)(maxActiveBlocks * blockSize)
                / deviceProp.maxThreadsPerMultiProcessor;

// 自动选择最佳 Block 大小
int minGridSize, optimalBlockSize;
cudaOccupancyMaxPotentialBlockSize(
    &minGridSize,
    &optimalBlockSize,
    myKernel,
    0,    // 动态 Shared Memory
    0     // Block 大小上限 (0 = 不限制)
);
```

`cudaOccupancyMaxPotentialBlockSize` 特别有用——它会自动搜索使 Occupancy 最大化的 Block 大小，省去手动试验的过程。

---

## 4.10 本章总结

### 核心概念速查

| 概念 | 要点 |
|------|------|
| **SM** | GPU 的独立处理器，包含计算单元、寄存器、Shared Memory、Warp Scheduler |
| **Warp** | 32 个线程的执行单位，同一 Warp 执行同一指令（SIMT） |
| **Warp Divergence** | Warp 内线程走不同分支 → 分支串行化 → 性能下降 |
| **延迟隐藏** | 用大量 Warp 填满内存等待时间，SM 始终在执行 |
| **Occupancy** | 活跃 Warp 数 / SM 最大 Warp 数，衡量延迟隐藏能力 |
| **资源限制** | 寄存器、Shared Memory、Thread Slot、Block Slot 共同决定 Occupancy |
| **Block 映射** | Block 由全局调度器分配到 SM，执行顺序不确定 |

### 性能分析思维框架

当你的 Kernel 性能不好时，按以下顺序排查：

```
GPU 性能排查清单:

1. Occupancy 够高吗？
   → 用 Occupancy Calculator 检查
   → 如果 < 50%, 找出瓶颈资源 (寄存器/Shared Memory/Block Size)

2. 有 Warp Divergence 吗？
   → 用 Nsight Compute 检查 "Branch Efficiency"
   → 尽量让分支以 Warp 为边界对齐

3. 内存访问模式好吗？ (下一章)
   → 是否合并访问？是否有 Bank Conflict？

4. 计算强度够高吗？
   → Roofline 模型分析：是内存受限还是计算受限？
   → 如果内存受限，考虑 Tiling 提高数据复用
```

### 硬件数字速查

| 参数 | V100 | A100 | H100 |
|------|------|------|------|
| SM 数 | 80 | 108 | 132 |
| Max Threads/SM | 2048 | 2048 | 2048 |
| Max Warps/SM | 64 | 64 | 64 |
| Max Blocks/SM | 32 | 32 | 32 |
| Registers/SM | 64K | 64K | 64K |
| Max Shared Mem/SM | 96 KB | 164 KB | 228 KB |
| Warp Schedulers/SM | 4 | 4 | 4 |
| Warp Size | 32 | 32 | 32 |

---

::: tip 下一章预告
[Ch05: Memory Architecture and Data Locality](./ch05) 将深入 GPU 的内存层次——全局内存、Shared Memory、常量内存、寄存器——以及核心优化技术 **Tiling**。我们将把第 3 章中性能只有峰值 1-5% 的基础矩阵乘法，用 Shared Memory Tiling 优化到 **30-50%** 的峰值利用率。
:::

---

## 扩展思考

::: details 思考题 1：如果一个 Kernel 只启动了 1 个 Block（32 个线程 = 1 个 Warp），GPU 的利用率是多少？
假设 A100（108 SMs，每 SM 64 Warps）：

- 1 个 Block 被分配到 **1 个 SM** 上
- 这个 SM 上只有 **1 个 Warp**
- 其余 **107 个 SM 完全空闲**

SM 利用率：1/108 ≈ 0.9%
Occupancy（该 SM）：1/64 ≈ 1.6%

这个 Kernel 只用了 GPU 千分之一的能力。这就是为什么 CUDA Kernel 必须启动大量线程——至少要让所有 SM 都有事做（>= SM 数量个 Block），而且每个 SM 上要有足够的 Warp 来隐藏延迟。

经验法则：一个 Kernel 应该至少启动 **数万** 个线程才有可能充分利用 GPU。
:::

::: details 思考题 2：为什么 Warp 大小是 32 而不是 64 或 16？
32 是 NVIDIA 多代架构实验后的工程平衡点：

**太小（如 16）**：
- 需要更多 Warp 才能达到同样的 Occupancy
- Warp Scheduler 的调度开销相对增大
- 内存合并访问的粒度不够（16 × 4B = 64B < 128B cache line）

**太大（如 64）**：
- Warp Divergence 影响更大（64 个线程中只要 1 个走不同分支就全部受影响）
- 需要更大的寄存器文件来支撑
- Block 大小最少是 64，灵活性下降

**32 的优势**：
- 32 × 4B = 128B = 恰好一个 cache line，内存合并完美匹配
- Divergence 影响在可控范围内
- 编程模型复杂度适中

注意：AMD GPU（RDNA 架构）的 Wave 大小是 32 或 64 可选，Intel GPU 的 Subgroup 大小是 8/16/32 可选——不同厂商的选择不同。
:::

::: details 思考题 3：`__launch_bounds__` 如何影响编译器行为？
`__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)` 给编译器两个提示：

1. **maxThreadsPerBlock**：告诉编译器这个 Kernel 永远不会用超过这么多线程启动
   - 编译器可以据此更激进地分配寄存器（因为不需要考虑更大 Block 的情况）

2. **minBlocksPerMultiprocessor**：告诉编译器要保证每 SM 至少能驻留这么多 Block
   - 编译器会限制寄存器使用量，确保 `regs × maxThreadsPerBlock × minBlocks ≤ 65536`
   - 如果不够，编译器会将部分变量 **spill** 到 local memory（很慢）

这是一个典型的 **寄存器数量 vs 寄存器 spill** 的权衡：
- 更多寄存器 → 单线程更快（避免 spill）→ 但 Occupancy 更低
- 更少寄存器 → Occupancy 更高 → 但可能有 spill 到全局内存的开销

最佳值需要实际 profiling 来确定。
:::

::: details 思考题 4：在 FlashAttention 等高性能 Kernel 中，Occupancy 通常是多少？
FlashAttention 的 Kernel 典型 Occupancy 只有 **25-50%**——远低于 100%，但性能极好。原因是：

1. **FlashAttention 是计算密集型**：大量的矩阵乘法运算（使用 Tensor Core），计算强度很高
2. **每线程使用大量寄存器**：为了把中间结果（如 softmax 的 running max 和 running sum）保存在寄存器中，避免写回全局内存
3. **大量 Shared Memory**：用于 Tiling Q、K、V 子块
4. **Tiling 策略**：每个 Block 处理一大块数据，Block 内计算密度极高

FlashAttention 的设计哲学是：**宁可 Occupancy 低一些，也要让每个线程的寄存器和 Shared Memory 充足，避免任何全局内存的冗余访问**。它用"每个线程做更多工作"替代了"用更多线程隐藏延迟"。

这说明 Occupancy 只是手段，不是目的。真正的目标是 **最大化有效带宽或计算吞吐**。
:::
